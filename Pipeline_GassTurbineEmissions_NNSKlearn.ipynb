{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "In this notebook we train a simple neural network with one hidden layer together with hyper parameter optimization for scaling method used, number of hidden layer neurons, hidden layer activation function, learning rate, number of epochs and the momentum to predict the CO gas emission from turbine sensor data. The original work was carried out in https://journals.tubitak.gov.tr/cgi/viewcontent.cgi?article=1505&context=elektrik with DOI: 10.3906/elk-1807-87.\n",
    "\n",
    "We make use of Azure Machine Learning (AzureML) cloud platform for training, testing and deployment of the model. We were able to achive a slightly higher validation and testing performance in terms of coeeffcient of determination, than what was mentioned in the original paper.\n",
    "\n",
    "### Prerequisites:\n",
    "1. An Azure account with an active subscription together with Owner or Contributer role.\n",
    "2. An AzureML Workspace with all required dependancies.\n",
    "3. Azure Machine Learning Python SDK v2 in Notebook execution environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Connect to Azure Machine Learning Workspace Using the Config.json file\n",
    "\n",
    "First, **DefaultCredential** is used and if it fails then use **InteractiveBrowserCredential** which will request Azure username and password in the browser. Once the credential are created, an object of MLClient is created using the Config.json file which is accessible directly from azure compute resources assiociated with a workspace. The Config.json file can also be downloaded from the AzureML studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532145991
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredentiall, InteractiveBrowserCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "ml_client = MLClient.from_config(credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create or Get a Reference to a Compute Cluster for Training and Deployment\n",
    "\n",
    "First, the workspace is checked for any compute resource with the given name and if exists a reference is created for that compute resource. If not then a compute cluster with size \"Standard_E8s_v3\" with default settings is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532146805
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "compute_cluster_name = \"GasEmission-cc-bhathiya\"\n",
    "try:\n",
    "    # Try to get a reference to a compute resource with the name\n",
    "    compute_cluster = ml_client.compute.get(name = compute_cluster_name)\n",
    "    print(f\"Compute-cluster with name {compute_cluster_name} already exists\")\n",
    "except:\n",
    "    # If not create a compute cluster with the name\n",
    "    compute_cluster = AmlCompute(\n",
    "        name=compute_cluster_name,\n",
    "        size=\"Standard_E8s_v3\",\n",
    "        idle_time_before_scale_down=180,\n",
    "        )\n",
    "    print(f\"Compute-cluster with name {compute_cluster_name} is being created\")\n",
    "    ml_client.compute.begin_create_or_update(compute_cluster).wait()\n",
    "    print(f\"Compute-cluster with name {compute_cluster_name} succesfuly created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download Training and Testing Data in to a Local Folder Structure.\n",
    "\n",
    "Sensor data from years 2011 to 2015 can be downloaded from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/551/gas+turbine+co+and+nox+emission+data+set). According the the orginal research paper, the first three years (2011, 2012, 2013) were used for training and the last two years (2014, 2015) were for testing. For performance comparison purposes, same train-test split is used.\n",
    "\n",
    "The code checks whether the data already exist in the required folder structure and if it does not, the data .zip file is downloaded from the repository and extracted locally. Trarning data is moved to \"./Data/PipelineNN/Train/\" folder and the testing data is moved to \"./Data/PipelineNN/Test/\" folder. The downloaded .zip file is then deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532147330
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "if (\n",
    "    # Check whther the data files exist in the local folder structure\n",
    "    os.path.exists(\"./Data/PipelineNN/Train/gt_2011.csv\") and\n",
    "    os.path.exists(\"./Data/PipelineNN/Train/gt_2012.csv\") and\n",
    "    os.path.exists(\"./Data/PipelineNN/Train/gt_2013.csv\") and\n",
    "    os.path.exists(\"./Data/PipelineNN/Test/gt_2014.csv\") and\n",
    "    os.path.exists(\"./Data/PipelineNN/Test/gt_2015.csv\")\n",
    "    ):\n",
    "    print(\"Data files already exist\")\n",
    "else:\n",
    "    # Data files do not exists!\n",
    "    \n",
    "    # Get s handle to the repository .zip file\n",
    "    print(\"Data files need to be downloaded\")\n",
    "    zipfile_handle = requests.get(\"https://archive.ics.uci.edu/static/public/551/gas+turbine+co+and+nox+emission+data+set.zip\", allow_redirects=True)\n",
    "    \n",
    "    # Download the .zip file to Data folder\n",
    "    local_dir_Data = \"./Data\"\n",
    "    if not os.path.exists(local_dir_Data):\n",
    "        os.mkdir(local_dir_Data)\n",
    "    with open(\"./Data/GasEmission.zip\",'wb') as output_file:\n",
    "        output_file.write(zipfile_handle.content)\n",
    "    \n",
    "    # Try to extract the downloaded .zip file\n",
    "    try:\n",
    "        with ZipFile(\"./Data/GasEmission.zip\", 'r') as zipfile_Object:\n",
    "            zipfile_Object.extractall(path=\"./Data\")\n",
    "            Extracted_bool = True\n",
    "            print(\"Zip file succusfuly extracted\")\n",
    "    except:\n",
    "        Extracted_bool = False\n",
    "        print(\"Downloaded Zipfile Error! Extraction Failed!\")\n",
    "    \n",
    "    # If extraction successful, move training file to Train folder and testing files to Test folder.\n",
    "    if (Extracted_bool):\n",
    "        local_dir_PipelineNN = \"./Data/PipelineNN\"\n",
    "        if not os.path.exists(local_dir_PipelineNN):\n",
    "            os.mkdir(local_dir_PipelineNN)\n",
    "\n",
    "        local_dir_Train = \"./Data/PipelineNN/Train\"\n",
    "        if not os.path.exists(local_dir_Train):\n",
    "            os.mkdir(local_dir_Train)\n",
    "\n",
    "        os.replace(\"./Data/gt_2011.csv\",\"./Data/PipelineNN/Train/gt_2011.csv\")\n",
    "        os.replace(\"./Data/gt_2012.csv\",\"./Data/PipelineNN/Train/gt_2012.csv\")\n",
    "        os.replace(\"./Data/gt_2013.csv\",\"./Data/PipelineNN/Train/gt_2013.csv\")\n",
    "        print(\"Succcesfuly created ./Data/PipelineNN/Train folder\")\n",
    "\n",
    "        local_dir_Test = \"./Data/PipelineNN/Test\"\n",
    "        if not os.path.exists(local_dir_Test):\n",
    "            os.mkdir(local_dir_Test)\n",
    "\n",
    "        os.replace(\"./Data/gt_2014.csv\",\"./Data/PipelineNN/Test/gt_2014.csv\")\n",
    "        os.replace(\"./Data/gt_2015.csv\",\"./Data/PipelineNN/Test/gt_2015.csv\")\n",
    "        print(\"Succcesfuly created ./Data/PipelineNN/Test folder\")\n",
    "        \n",
    "        # Delete the downloaded .zip file\n",
    "        os.remove(\"./Data/GasEmission.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define and Create the Environment for Training, Testing and Deployment\n",
    "\n",
    "#### 4.1 Create a Folder to Hold Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532147825
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Environment Definition in .yaml Format\n",
    "\n",
    "ScikitLearn and TensorFlow both are added, so that the environment can be used for any model supported by ScikitLearn and Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {dependencies_dir}/conda_sk_tensor.yaml\n",
    "name: model-env\n",
    "channels:\n",
    "- anaconda\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.8\n",
    "- pip=21.3.1\n",
    "- pandas~=1.3.0\n",
    "- scipy~=1.7.0\n",
    "- numpy~=1.22.0\n",
    "- pip:\n",
    "  - scikit-learn-intelex==2023.1.1\n",
    "  - wheel~=0.38.1\n",
    "  - matplotlib~=3.5.0\n",
    "  - psutil~=5.8.0\n",
    "  - tqdm~=4.62.0\n",
    "  - ipykernel~=6.20.2\n",
    "  - azureml-core==1.50.0\n",
    "  - azureml-defaults==1.50.0\n",
    "  - azureml-mlflow==1.50.0\n",
    "  - azureml-telemetry==1.50.0\n",
    "  - scikit-learn~=1.1.0\n",
    "  - joblib~=1.2.0\n",
    "  - debugpy~=1.6.3\n",
    "  - tensorflow~=2.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Create and Register the Environment in the Workspace\n",
    "\n",
    "Uses the environment definition .yaml file written previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532148950
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"aml-scikit-learn_tensorflow\"\n",
    "custom_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for Gas Emission Prediction with Sklearn and Tensorflow\",\n",
    "    tags={\"scikit-learn\": \"1.1.0\", \"tensorflow\": \"2.12.0\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda_sk_tensor.yaml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04\"\n",
    ")\n",
    "ml_client.environments.create_or_update(custom_env)\n",
    "\n",
    "print(f\"Environment with name {custom_env.name} is registered to workspace, the environment version is {custom_env.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create and Run Training Pipeline\n",
    "\n",
    "#### 5.1 Create a Folder to Hold Component Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532149538
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "local_dir_Pipeline = \"./src/Pipeline_components\"\n",
    "if not os.path.exists(local_dir_Pipeline):\n",
    "    os.mkdir(local_dir_Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {local_dir_Pipeline}/combine_data.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_files\", type=str, help=\"path to input data folder\")\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to output train data folder\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to output test data folder\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    df_train_2011 = pd.read_csv(os.path.join(args.data_files, \"Train/gt_2011.csv\"))\n",
    "    df_train_2012 = pd.read_csv(os.path.join(args.data_files, \"Train/gt_2012.csv\"))\n",
    "    df_train_2013 = pd.read_csv(os.path.join(args.data_files, \"Train/gt_2013.csv\"))\n",
    "    \n",
    "    df_train_full = pd.concat([df_train_2011, df_train_2012, df_train_2013])\n",
    "\n",
    "    X_train_full = df_train_full.drop([\"CO\",\"NOX\"],axis=1)\n",
    "    y_train_full = df_train_full[[\"CO\"]]\n",
    "\n",
    "    X_train_full.to_csv(os.path.join(args.train_data,\"train_X.csv\"), index=False)\n",
    "    y_train_full.to_csv(os.path.join(args.train_data,\"train_Y.csv\"), index=False)\n",
    "\n",
    "    df_test_2014 = pd.read_csv(os.path.join(args.data_files, \"Test/gt_2014.csv\"))\n",
    "    df_test_2015 = pd.read_csv(os.path.join(args.data_files, \"Test/gt_2015.csv\"))\n",
    "    \n",
    "    df_test_full = pd.concat([df_test_2014, df_test_2015])\n",
    "\n",
    "    X_test_full = df_test_full.drop([\"CO\",\"NOX\"],axis=1)\n",
    "    y_test_full = df_test_full[[\"CO\"]]\n",
    "\n",
    "    X_test_full.to_csv(os.path.join(args.test_data,\"test_X.csv\"), index=False)\n",
    "    y_test_full.to_csv(os.path.join(args.test_data,\"test_Y.csv\"), index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {local_dir_Pipeline}/train_NNh1_sklearn.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--random_state\", type=int, required=False, default=0)\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"Path to training data\")\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "    parser.add_argument(\"--scaler_type\", type=str, required=False, default=\"minmax\")\n",
    "    parser.add_argument(\"--hidden_layer_neurons\", type=int, required=False, default=100)\n",
    "    parser.add_argument(\"--hidden_layer_activation\", type=str, required=False, default=\"relu\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, required=False, default=10)\n",
    "    parser.add_argument(\"--epochs\", type=int, required=False, default=30)\n",
    "    parser.add_argument(\"--momentum\", type=float, required=False, default=0)\n",
    "    parser.add_argument(\"--ouput_model_path\", type=str, help=\"Path for the model\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    X_full = pd.read_csv(os.path.join(args.train_data, \"train_X.csv\"))\n",
    "    y_full = pd.read_csv(os.path.join(args.train_data, \"train_Y.csv\"))\n",
    "\n",
    "    mlflow.start_run()\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    X_train, X_vali, y_train, y_vali =  train_test_split(X_full,y_full,test_size=args.test_train_ratio,random_state=args.random_state)\n",
    " \n",
    "    model_sklearnNN = MLPRegressor(\n",
    "        hidden_layer_sizes=args.hidden_layer_neurons, \n",
    "        activation=args.hidden_layer_activation, \n",
    "        solver=\"sgd\", \n",
    "        learning_rate=\"adaptive\", \n",
    "        learning_rate_init=args.learning_rate,  \n",
    "        max_iter=args.epochs,\n",
    "        momentum=args.momentum,\n",
    "        random_state=args.random_state\n",
    "        )\n",
    "\n",
    "    if (args.scaler_type == \"minmax\"):\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler_model_pipeline = Pipeline([\n",
    "            (\"scalar\", scaler),\n",
    "            (\"nn_model\", model_sklearnNN)\n",
    "            ])\n",
    "    elif (args.scaler_type == \"standard\"):\n",
    "        scaler = StandardScaler()\n",
    "        scaler_model_pipeline = Pipeline([\n",
    "            (\"scalar\", scaler),\n",
    "            (\"nn_model\", model_sklearnNN)\n",
    "            ])\n",
    "    elif (args.scaler_type == \"maxabs\"):\n",
    "        scaler = MaxAbsScaler()\n",
    "        scaler_model_pipeline = Pipeline([\n",
    "            (\"scalar\", scaler),\n",
    "            (\"nn_model\", model_sklearnNN)\n",
    "            ])\n",
    "    else:\n",
    "        scaler_model_pipeline = model_sklearnNN\n",
    "\n",
    "    scaler_model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_predict = scaler_model_pipeline.predict(X_train)\n",
    "    mlflow.log_metric(\"Train RMSE\",  np.sqrt(mean_squared_error(y_train,y_predict)))\n",
    "    mlflow.log_metric(\"Train R2-score\", r2_score(y_train,y_predict))\n",
    "\n",
    "    y_predict = scaler_model_pipeline.predict(X_vali)\n",
    "    mlflow.log_metric(\"Validation RMSE\",  np.sqrt(mean_squared_error(y_vali,y_predict)))\n",
    "    mlflow.log_metric(\"Validation R2-score\", r2_score(y_vali,y_predict))\n",
    "\n",
    "    # Saving the model to a file\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=scaler_model_pipeline,\n",
    "        path=args.ouput_model_path,\n",
    "    )\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {local_dir_Pipeline}/test_and_register_model_nnsklearn.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import mlflow.sklearn\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--test_data_folder\", type=str, help=\"Path of the test data folder\")\n",
    "    parser.add_argument(\"--input_model_path\", type=str, help=\"path of the input model\")\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"Name for the registered best model\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    X_test = pd.read_csv(os.path.join(args.test_data_folder, \"test_X.csv\"))\n",
    "    y_test = pd.read_csv(os.path.join(args.test_data_folder, \"test_Y.csv\"))\n",
    "\n",
    "    mlflow.start_run()\n",
    "    \n",
    "    loaded_best_model = mlflow.sklearn.load_model(args.input_model_path)\n",
    "\n",
    "    print(\"Registering the best sweeped model via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=loaded_best_model,\n",
    "        registered_model_name=args.registered_model_name,\n",
    "        artifact_path=args.registered_model_name,\n",
    "    )\n",
    "\n",
    "    y_predict = loaded_best_model.predict(X_test)\n",
    "    mlflow.log_metric(\"Test RMSE\",  np.sqrt(mean_squared_error(y_test,y_predict)))\n",
    "    mlflow.log_metric(\"Test R2-score\", r2_score(y_test,y_predict))\n",
    "\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532156362
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "combine_data_component = command(\n",
    "    name=\"combine_files_for_train_test\",\n",
    "    display_name=\"combine_files_for_train_test\",\n",
    "    description=\"Combine .csv files for training and testing seperatly\",\n",
    "    inputs={\n",
    "        \"data_files\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    \n",
    "    code=local_dir_Pipeline,\n",
    "    command=\"python combine_data.py \\\n",
    "            --data_files ${{inputs.data_files}} \\\n",
    "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
    "            \",\n",
    "    environment=f\"{custom_env.name}:{custom_env.version}\"\n",
    "    #environment = \"azureml://registries/azureml/environments/responsibleai-ubuntu20.04-py38-cpu/versions/21\"\n",
    ")\n",
    "combine_data_component = ml_client.create_or_update(combine_data_component.component)\n",
    "\n",
    "train_nnh1_component = command(\n",
    "    name=\"train_nnh1_regressor\",\n",
    "    display_name=\"train_nnh1_regressor\",\n",
    "    inputs={\n",
    "        \"random_state\": Input(type=\"number\"),\n",
    "        \"train_data\": Input(type=\"uri_folder\"),\n",
    "        \"test_train_ratio\": Input(type=\"number\"),\n",
    "        \"scaler_type\": Input(type=\"string\"),\n",
    "        \"hidden_layer_neurons\": Input(type=\"number\"),\n",
    "        \"hidden_layer_activation\": Input(type=\"string\"),\n",
    "        \"learning_rate\": Input(type=\"number\"),\n",
    "        \"epochs\": Input(type=\"number\"),\n",
    "        \"momentum\": Input(type=\"number\")\n",
    "    },\n",
    "    outputs={\n",
    "        \"ouput_model_path\": Output(type=\"mlflow_model\")\n",
    "    },\n",
    "    \n",
    "    code=local_dir_Pipeline,\n",
    "    command=\"python train_NNh1_sklearn.py \\\n",
    "            --random_state ${{inputs.random_state}} \\\n",
    "            --train_data ${{inputs.train_data}} \\\n",
    "            --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
    "            --scaler_type ${{inputs.scaler_type}}\\\n",
    "            --hidden_layer_neurons ${{inputs.hidden_layer_neurons}} \\\n",
    "            --hidden_layer_activation ${{inputs.hidden_layer_activation}} \\\n",
    "            --learning_rate ${{inputs.learning_rate}} \\\n",
    "            --epochs ${{inputs.epochs}} \\\n",
    "            --momentum ${{inputs.momentum}}\\\n",
    "            --ouput_model_path ${{outputs.ouput_model_path}} \\\n",
    "            \",\n",
    "    environment=f\"{custom_env.name}:{custom_env.version}\"\n",
    "    #environment = \"azureml://registries/azureml/environments/responsibleai-ubuntu20.04-py38-cpu/versions/21\"\n",
    ")\n",
    "train_nnh1_component = ml_client.create_or_update(train_nnh1_component.component)\n",
    "\n",
    "test_and_register_model_component = command(\n",
    "    name=\"test_model\",\n",
    "    display_name=\"test_model\",\n",
    "    inputs={\n",
    "        \"test_data_folder\": Input(type=\"uri_folder\"),\n",
    "        \"input_model_path\": Input(type=\"mlflow_model\"),\n",
    "        \"registered_model_name\": Input(type=\"string\")\n",
    "    },\n",
    "    \n",
    "    code=local_dir_Pipeline,\n",
    "    command=\"python test_and_register_model_nnsklearn.py \\\n",
    "            --test_data_folder ${{inputs.test_data_folder}} \\\n",
    "            --input_model_path ${{inputs.input_model_path}} \\\n",
    "            --registered_model_name ${{inputs.registered_model_name}} \\\n",
    "            \",\n",
    "    environment=f\"{custom_env.name}:{custom_env.version}\"\n",
    "    #environment = \"azureml://registries/azureml/environments/responsibleai-ubuntu20.04-py38-cpu/versions/21\"\n",
    ")\n",
    "test_and_register_model_component = ml_client.create_or_update(test_and_register_model_component.component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532156879
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl\n",
    "from azure.ai.ml.sweep import Choice\n",
    "import numpy as np\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=compute_cluster_name, # to use serverless compute, change this to: compute=azureml:serverless\n",
    "    description=\"Gas Emission Prediction Pipeline - NNh1\",\n",
    ")\n",
    "def GassEmission_prediction_pipeline_nnh1_sklearn(\n",
    "    pipeline_job_input_data_folder,\n",
    "    pipeline_job_test_train_ratio,\n",
    "    pipeline_job_registered_model_name,\n",
    "    pipeline_job_random_state\n",
    "):\n",
    "    \n",
    "    combine_data_job = combine_data_component(\n",
    "        data_files=pipeline_job_input_data_folder\n",
    "    )\n",
    "\n",
    "    train_job = train_nnh1_component(\n",
    "        random_state=pipeline_job_random_state,\n",
    "        train_data=combine_data_job.outputs.train_data,\n",
    "        test_train_ratio=pipeline_job_test_train_ratio,\n",
    "        scaler_type=Choice(values=[\"minmax\", \"none\"]),\n",
    "        hidden_layer_neurons=50,\n",
    "        hidden_layer_activation=\"relu\",\n",
    "        learning_rate=Choice(values=[0.1, 0.2]),\n",
    "        epochs=Choice(values=[160, 240]),\n",
    "        momentum=0\n",
    "    )\n",
    "\n",
    "    sweep_step = train_job.sweep(\n",
    "        sampling_algorithm=\"grid\",\n",
    "        primary_metric=\"Validation R2-score\",\n",
    "        goal=\"maximize\"\n",
    "    )\n",
    "    sweep_step.set_limits(max_total_trials=500, max_concurrent_trials=20, timeout=72000)\n",
    "\n",
    "    test_and_register_model_job = test_and_register_model_component(\n",
    "        test_data_folder=combine_data_job.outputs.test_data,\n",
    "        registered_model_name=pipeline_job_registered_model_name,\n",
    "        input_model_path=sweep_step.outputs.ouput_model_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532857294
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "registered_best_model_name = \"pipeline_best_model_nnsklearn\"\n",
    "\n",
    "pipeline = GassEmission_prediction_pipeline_nnh1_sklearn(\n",
    "    pipeline_job_random_state=0,\n",
    "    pipeline_job_input_data_folder=Input(type=\"uri_folder\", path=\"./Data/PipelineNN\"),\n",
    "    pipeline_job_test_train_ratio=0.2,\n",
    "    pipeline_job_registered_model_name=registered_best_model_name\n",
    ")\n",
    "\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    experiment_name=\"train_model_GasEmission_prediction_Pipeline_nnh1_sklearn\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532859169
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "latest_model_version = max(\n",
    "    [int(m.version) for m in ml_client.models.list(name=registered_best_model_name)]\n",
    ")\n",
    "registered_best_model = ml_client.models.get(name=registered_best_model_name,version=latest_model_version)\n",
    "\n",
    "best_model_job = ml_client.jobs.get(name=registered_best_model.job_name)\n",
    "\n",
    "MLflow_client = MlflowClient()\n",
    "mlflow_best_model_job = MLflow_client.get_run(best_model_job.name)\n",
    "mlflow_best_model_job_parent = MLflow_client.get_run(mlflow_best_model_job.data.tags[\"mlflow.parentRunId\"])\n",
    "\n",
    "if (mlflow_best_model_job_parent.data.tags[\"mlflow.rootRunId\"] != pipeline_job.name):\n",
    "    print(\"Registered Best Model Version Conflict Detected! \\nPipeline_job runID and the best model's associated job's parent job's runID are mismatched. The model may be updated by some other job\")\n",
    "    registered_best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532859617
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(\"Registered Best Model Name: \", registered_best_model.name)\n",
    "print(\"Registered Best Model Version: \", registered_best_model.version)\n",
    "print(\"Registered Best Model Path: \", registered_best_model.path)\n",
    "print(\"Registered Best Model ID: \", registered_best_model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532860244
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "registry_name = \"azureml\"\n",
    "ml_client_registry = MLClient(\n",
    "   credential=DefaultAzureCredential(),\n",
    "    registry_name=registry_name,\n",
    ")\n",
    "\n",
    "rai_constructor_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_insight_constructor\", label=\"latest\"\n",
    ")\n",
    "rai_version = rai_constructor_component.version\n",
    "\n",
    "rai_explanation_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_explanation\", version=rai_version\n",
    ")\n",
    "\n",
    "rai_erroranalysis_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_erroranalysis\", version=rai_version\n",
    ")\n",
    "\n",
    "rai_gather_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_insight_gather\", version=rai_version\n",
    ")\n",
    "\n",
    "rai_scorecard_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_score_card\", version=rai_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532860595
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "score_card_config_dict = {\n",
    "    \"Model\": {\n",
    "        \"ModelName\": \"Gas Emission Prediction Model\",\n",
    "        \"ModelType\": \"Regression\",\n",
    "        \"ModelSummary\": \"This model provides predictions for CO gas emission in a tubine using sensor data\",\n",
    "    },\n",
    "    \"Metrics\": {\"mean_absolute_error\": {\"threshold\": \"<=5\"}, \"mean_squared_error\": {}},\n",
    "}\n",
    "\n",
    "local_dir_rai = \"./RAI\"\n",
    "if not os.path.exists(local_dir_rai):\n",
    "    os.mkdir(local_dir_rai)\n",
    "score_card_config_filename = \"./RAI/rai_gas_emission_prediction_score_card_config.json\"\n",
    "\n",
    "with open(score_card_config_filename, \"w\") as f:\n",
    "    json.dump(score_card_config_dict, f)\n",
    "\n",
    "score_card_config_as_Input = Input(type=\"uri_file\", path=score_card_config_filename, mode=\"download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532861081
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "\n",
    "GasEmission_train_Data = Input(\n",
    "    type=\"mltable\",\n",
    "    path=\"azureml:RAI_GasEmission_Train:1\",\n",
    "    mode=\"download\",\n",
    ")\n",
    "GasEmission_test_Data = Input(\n",
    "    type=\"mltable\",\n",
    "    path=\"azureml:RAI_GasEmission_Test:1\",\n",
    "    mode=\"download\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532861367
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=compute_cluster_name,\n",
    "    experiment_name=\"RAI_GasEmissionPrediction_SKlearn_NNh1\",\n",
    ")\n",
    "def rai_regression_pipeline(\n",
    "    target_column_name,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    score_card_config,\n",
    "):\n",
    "    # Initiate the RAIInsights\n",
    "    create_rai_job = rai_constructor_component(\n",
    "        title=\"RAI Dashboard - Gas Emission Prediction - SKLearn NNh1\",\n",
    "        task_type=\"regression\",\n",
    "        model_info=f\"{registered_best_model.name}:{registered_best_model.version}\",\n",
    "        model_input=Input(type=\"mlflow_model\", path=f\"azureml:{registered_best_model.name}:{registered_best_model.version}\"),\n",
    "        train_dataset=train_data,\n",
    "        test_dataset=test_data,\n",
    "        target_column_name=target_column_name,\n",
    "        use_model_dependency=False,\n",
    "    )\n",
    "\n",
    "    explain_job = rai_explanation_component(\n",
    "        comment=\"Explanation for the Gas Prediction Model\",\n",
    "        rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "    )\n",
    "\n",
    "    erroranalysis_job = rai_erroranalysis_component(\n",
    "        rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "    )\n",
    "\n",
    "    rai_gather_job = rai_gather_component(\n",
    "        constructor=create_rai_job.outputs.rai_insights_dashboard,\n",
    "        insight_1=explain_job.outputs.explanation,\n",
    "        insight_2=erroranalysis_job.outputs.error_analysis,\n",
    "    )\n",
    "    rai_gather_job.outputs.dashboard.mode = \"upload\"\n",
    "    rai_gather_job.outputs.ux_json.mode = \"upload\"\n",
    "\n",
    "    # Generate score card in pdf format for a summary report on model performance,\n",
    "    # and observe distrbution of error between prediction vs ground truth.\n",
    "    rai_scorecard_job = rai_scorecard_component(\n",
    "        dashboard=rai_gather_job.outputs.dashboard,\n",
    "        pdf_generation_config=score_card_config_as_Input,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"dashboard\": rai_gather_job.outputs.dashboard,\n",
    "        \"ux_json\": rai_gather_job.outputs.ux_json,\n",
    "        \"scorecard\": rai_scorecard_job.outputs.scorecard,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687534849689
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from azure.ai.ml import Output\n",
    "\n",
    "# Pipeline to construct the RAI Insights\n",
    "insights_pipeline_job = rai_regression_pipeline(\n",
    "    target_column_name=\"CO\",\n",
    "    train_data=GasEmission_train_Data,\n",
    "    test_data=GasEmission_test_Data,\n",
    "    score_card_config=score_card_config_as_Input,\n",
    ")\n",
    "\n",
    "# Workaround to enable the download\n",
    "rand_path = str(uuid.uuid4())\n",
    "insights_pipeline_job.outputs.dashboard = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/dashboard/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")\n",
    "insights_pipeline_job.outputs.ux_json = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/ux_json/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")\n",
    "insights_pipeline_job.outputs.scorecard = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/scorecard/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")\n",
    "\n",
    "rai_job = ml_client.jobs.create_or_update(\n",
    "    insights_pipeline_job,\n",
    "    experiment_name=\"rai_GasEmission_prediction_sklearn_nnh1\",\n",
    ")\n",
    "ml_client.jobs.stream(rai_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687536570116
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "endpoint_name = \"gaspredict-nnsklearn-\" + str(uuid.uuid4())[:8]\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"This is an online endpoint for CO gas emission prediction - nnsklearn\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\n",
    "        \"training_dataset\": \"sensor_data\",\n",
    "        \"model_type\": \"sklearn.MLPRegressor\",\n",
    "    },\n",
    ")\n",
    "\n",
    "endpoint_result = ml_client.begin_create_or_update(endpoint).result()\n",
    "\n",
    "print(f\"Endpint {endpoint_result.name} provisioning state: {endpoint_result.provisioning_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687537314921
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=registered_best_model,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "blue_deployment_results = ml_client.online_deployments.begin_create_or_update(\n",
    "    blue_deployment\n",
    ").result()\n",
    "\n",
    "print(\n",
    "    f\"Deployment {blue_deployment_results.name} provisioning state: {blue_deployment_results.provisioning_state}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "deploy_dir = \"./deploy\"\n",
    "if not os.path.exists(deploy_dir):\n",
    "    os.mkdir(deploy_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {deploy_dir}/sample-request.json\n",
    "{\n",
    "  \"input_data\": {\n",
    "    \"columns\": [\"AT\", \"AP\", \"AH\", \"AFDP\", \"GTEP\", \"TIT\", \"TAT\", \"TEY\", \"CDP\"],\n",
    "    \"index\": [0, 1],\n",
    "    \"data\": [\n",
    "            [1.9532,1020.1,84.985,2.5304,20.116,1048.7,544.92,116.27,10.799],\n",
    "            [1.2191,1020.1,87.523,2.3937,18.584,1045.5,548.5,109.18,10.347]\n",
    "        ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687539692801
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=endpoint_name,\n",
    "    request_file=\"./deploy/sample-request.json\",\n",
    "    deployment_name=\"blue\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
