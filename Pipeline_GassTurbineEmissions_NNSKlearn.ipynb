{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "In this notebook we train a simple neural network with one hidden layer together with hyper parameter optimization for scaling method used, number of hidden layer neurons, hidden layer activation function, learning rate, number of epochs and the momentum to predict the CO gas emission from turbine sensor data. The original work was carried out in https://journals.tubitak.gov.tr/cgi/viewcontent.cgi?article=1505&context=elektrik with DOI: 10.3906/elk-1807-87.\n",
    "\n",
    "We make use of Azure Machine Learning (AzureML) cloud platform for training, testing and deployment of the model. We were able to achive a slightly higher validation and testing performance in terms of coeeffcient of determination, than what was mentioned in the original paper.\n",
    "\n",
    "### Prerequisites:\n",
    "1. An Azure account with an active subscription together with Owner or Contributer role.\n",
    "2. An AzureML Workspace with all required dependancies.\n",
    "3. Azure Machine Learning Python SDK v2 in Notebook execution environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Connect to Azure Machine Learning Workspace Using the Config.json file\n",
    "\n",
    "First, **DefaultCredential** is used and if it fails then use **InteractiveBrowserCredential** which will request Azure username and password in the browser. Once the credential are created, an object of MLClient is created using the Config.json file which is accessible directly from azure compute resources assiociated with a workspace. The Config.json file can also be downloaded from the AzureML studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532145991
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredentiall, InteractiveBrowserCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "ml_client = MLClient.from_config(credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create or Get a Reference to a Compute Cluster for Training and Deployment\n",
    "\n",
    "First, the workspace is checked for any compute resource with the given name and if exists a reference is created for that compute resource. If not then a compute cluster with size \"Standard_E8s_v3\" with default settings is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532146805
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "compute_cluster_name = \"GasEmission-cc-bhathiya\"\n",
    "try:\n",
    "    # Try to get a reference to a compute resource with the name\n",
    "    compute_cluster = ml_client.compute.get(name = compute_cluster_name)\n",
    "    print(f\"Compute-cluster with name {compute_cluster_name} already exists\")\n",
    "except:\n",
    "    # If not create a compute cluster with the name\n",
    "    compute_cluster = AmlCompute(\n",
    "        name=compute_cluster_name,\n",
    "        size=\"Standard_E8s_v3\",\n",
    "        idle_time_before_scale_down=180,\n",
    "        )\n",
    "    print(f\"Compute-cluster with name {compute_cluster_name} is being created\")\n",
    "    ml_client.compute.begin_create_or_update(compute_cluster).wait()\n",
    "    print(f\"Compute-cluster with name {compute_cluster_name} succesfuly created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download Training and Testing Data in to a Local Folder Structure.\n",
    "\n",
    "Sensor data from years 2011 to 2015 can be downloaded from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/551/gas+turbine+co+and+nox+emission+data+set). According the the orginal research paper, the first three years (2011, 2012, 2013) were used for training and the last two years (2014, 2015) were for testing. For performance comparison purposes, same train-test split is used.\n",
    "\n",
    "The code checks whether the data already exist in the required folder structure and if it does not, the data .zip file is downloaded from the repository and extracted locally. Trarning data is moved to \"./Data/PipelineNN/Train/\" folder and the testing data is moved to \"./Data/PipelineNN/Test/\" folder. The downloaded .zip file is then deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532147330
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "if (\n",
    "    # Check whther the data files exist in the local folder structure\n",
    "    os.path.exists(\"./Data/PipelineNN/Train/gt_2011.csv\") and\n",
    "    os.path.exists(\"./Data/PipelineNN/Train/gt_2012.csv\") and\n",
    "    os.path.exists(\"./Data/PipelineNN/Train/gt_2013.csv\") and\n",
    "    os.path.exists(\"./Data/PipelineNN/Test/gt_2014.csv\") and\n",
    "    os.path.exists(\"./Data/PipelineNN/Test/gt_2015.csv\")\n",
    "    ):\n",
    "    print(\"Data files already exist\")\n",
    "else:\n",
    "    # Data files do not exists!\n",
    "    \n",
    "    # Get s handle to the repository .zip file\n",
    "    print(\"Data files need to be downloaded\")\n",
    "    zipfile_handle = requests.get(\"https://archive.ics.uci.edu/static/public/551/gas+turbine+co+and+nox+emission+data+set.zip\", allow_redirects=True)\n",
    "    \n",
    "    # Download the .zip file to Data folder\n",
    "    local_dir_Data = \"./Data\"\n",
    "    if not os.path.exists(local_dir_Data):\n",
    "        os.mkdir(local_dir_Data)\n",
    "    with open(\"./Data/GasEmission.zip\",'wb') as output_file:\n",
    "        output_file.write(zipfile_handle.content)\n",
    "    \n",
    "    # Try to extract the downloaded .zip file\n",
    "    try:\n",
    "        with ZipFile(\"./Data/GasEmission.zip\", 'r') as zipfile_Object:\n",
    "            zipfile_Object.extractall(path=\"./Data\")\n",
    "            Extracted_bool = True\n",
    "            print(\"Zip file succusfuly extracted\")\n",
    "    except:\n",
    "        Extracted_bool = False\n",
    "        print(\"Downloaded Zipfile Error! Extraction Failed!\")\n",
    "    \n",
    "    # If extraction successful, move training file to Train folder and testing files to Test folder.\n",
    "    if (Extracted_bool):\n",
    "        local_dir_PipelineNN = \"./Data/PipelineNN\"\n",
    "        if not os.path.exists(local_dir_PipelineNN):\n",
    "            os.mkdir(local_dir_PipelineNN)\n",
    "\n",
    "        local_dir_Train = \"./Data/PipelineNN/Train\"\n",
    "        if not os.path.exists(local_dir_Train):\n",
    "            os.mkdir(local_dir_Train)\n",
    "\n",
    "        os.replace(\"./Data/gt_2011.csv\",\"./Data/PipelineNN/Train/gt_2011.csv\")\n",
    "        os.replace(\"./Data/gt_2012.csv\",\"./Data/PipelineNN/Train/gt_2012.csv\")\n",
    "        os.replace(\"./Data/gt_2013.csv\",\"./Data/PipelineNN/Train/gt_2013.csv\")\n",
    "        print(\"Succcesfuly created ./Data/PipelineNN/Train folder\")\n",
    "\n",
    "        local_dir_Test = \"./Data/PipelineNN/Test\"\n",
    "        if not os.path.exists(local_dir_Test):\n",
    "            os.mkdir(local_dir_Test)\n",
    "\n",
    "        os.replace(\"./Data/gt_2014.csv\",\"./Data/PipelineNN/Test/gt_2014.csv\")\n",
    "        os.replace(\"./Data/gt_2015.csv\",\"./Data/PipelineNN/Test/gt_2015.csv\")\n",
    "        print(\"Succcesfuly created ./Data/PipelineNN/Test folder\")\n",
    "        \n",
    "        # Delete the downloaded .zip file\n",
    "        os.remove(\"./Data/GasEmission.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define and Create the Environment for Training, Testing and Deployment\n",
    "\n",
    "#### 4.1 Create a Folder to Hold Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532147825
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Environment Definition in .yaml Format\n",
    "\n",
    "ScikitLearn and TensorFlow both are added, so that the environment can be used for any model supported by ScikitLearn and Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {dependencies_dir}/conda_sk_tensor.yaml\n",
    "name: model-env\n",
    "channels:\n",
    "- anaconda\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.8\n",
    "- pip=21.3.1\n",
    "- pandas~=1.3.0\n",
    "- scipy~=1.7.0\n",
    "- numpy~=1.22.0\n",
    "- pip:\n",
    "  - scikit-learn-intelex==2023.1.1\n",
    "  - wheel~=0.38.1\n",
    "  - matplotlib~=3.5.0\n",
    "  - psutil~=5.8.0\n",
    "  - tqdm~=4.62.0\n",
    "  - ipykernel~=6.20.2\n",
    "  - azureml-core==1.50.0\n",
    "  - azureml-defaults==1.50.0\n",
    "  - azureml-mlflow==1.50.0\n",
    "  - azureml-telemetry==1.50.0\n",
    "  - scikit-learn~=1.1.0\n",
    "  - joblib~=1.2.0\n",
    "  - debugpy~=1.6.3\n",
    "  - tensorflow~=2.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Create and Register the Environment in the Workspace\n",
    "\n",
    "Uses the environment definition .yaml file written previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532148950
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"aml-scikit-learn_tensorflow\"\n",
    "custom_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for Gas Emission Prediction with Sklearn and Tensorflow\",\n",
    "    tags={\"scikit-learn\": \"1.1.0\", \"tensorflow\": \"2.12.0\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda_sk_tensor.yaml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04\"\n",
    ")\n",
    "ml_client.environments.create_or_update(custom_env)\n",
    "\n",
    "print(f\"Environment with name {custom_env.name} is registered to workspace, the environment version is {custom_env.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create and Run Training Pipeline\n",
    "\n",
    "#### 5.1 Create a Folder to Hold Component Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532149538
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "local_dir_Pipeline = \"./src/Pipeline_components\"\n",
    "if not os.path.exists(local_dir_Pipeline):\n",
    "    os.mkdir(local_dir_Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Code for the Component to Comine Data and Create Train and Test Datasets \n",
    "Train and Test datasets are created combining the data files contained in the input data folder \"data_files\". Creating the training and testing datasets is carried out accourding to the instructions given in the research paper as described in Section-3 in this Notebook. Train and test data are saved as .csv files in the folder path defined by \"train_data\" and \"test_data\" respectively. The testing dataset is kept as the holdout dataset, which is used to obtain the final performance results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {local_dir_Pipeline}/combine_data.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_files\", type=str, help=\"path to input data folder\")\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to output train data folder\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to output test data folder\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    df_train_2011 = pd.read_csv(os.path.join(args.data_files, \"Train/gt_2011.csv\"))\n",
    "    df_train_2012 = pd.read_csv(os.path.join(args.data_files, \"Train/gt_2012.csv\"))\n",
    "    df_train_2013 = pd.read_csv(os.path.join(args.data_files, \"Train/gt_2013.csv\"))\n",
    "    \n",
    "    df_train_full = pd.concat([df_train_2011, df_train_2012, df_train_2013])\n",
    "\n",
    "    X_train_full = df_train_full.drop([\"CO\",\"NOX\"],axis=1)\n",
    "    y_train_full = df_train_full[[\"CO\"]]\n",
    "\n",
    "    X_train_full.to_csv(os.path.join(args.train_data,\"train_X.csv\"), index=False)\n",
    "    y_train_full.to_csv(os.path.join(args.train_data,\"train_Y.csv\"), index=False)\n",
    "\n",
    "    df_test_2014 = pd.read_csv(os.path.join(args.data_files, \"Test/gt_2014.csv\"))\n",
    "    df_test_2015 = pd.read_csv(os.path.join(args.data_files, \"Test/gt_2015.csv\"))\n",
    "    \n",
    "    df_test_full = pd.concat([df_test_2014, df_test_2015])\n",
    "\n",
    "    X_test_full = df_test_full.drop([\"CO\",\"NOX\"],axis=1)\n",
    "    y_test_full = df_test_full[[\"CO\"]]\n",
    "\n",
    "    X_test_full.to_csv(os.path.join(args.test_data,\"test_X.csv\"), index=False)\n",
    "    y_test_full.to_csv(os.path.join(args.test_data,\"test_Y.csv\"), index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Code for the Component to Train and Save the Model\n",
    "\n",
    "The model consists of a scaler followed by a neural-network with single hidden layer. Model hyper-parameters are type of scaling, number of hidden layer neurons, hidden layer activation function, learning rate, number of epochs and the momentum. The training dataset is divided in to a training and a validation set. Validation RMSE and the R2 score is logged using MLFlow for hyper parameter tuning. Training RMSE and R2 score is also logged for bias-variance analysis, when needed. Training model pipeline is saved as a MLFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {local_dir_Pipeline}/train_NNh1_sklearn.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--random_state\", type=int, required=False, default=0)\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"Path to training data\")\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "    parser.add_argument(\"--scaler_type\", type=str, required=False, default=\"minmax\")\n",
    "    parser.add_argument(\"--hidden_layer_neurons\", type=int, required=False, default=100)\n",
    "    parser.add_argument(\"--hidden_layer_activation\", type=str, required=False, default=\"relu\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, required=False, default=10)\n",
    "    parser.add_argument(\"--epochs\", type=int, required=False, default=30)\n",
    "    parser.add_argument(\"--momentum\", type=float, required=False, default=0)\n",
    "    parser.add_argument(\"--ouput_model_path\", type=str, help=\"Path for the model\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #Load training set into Dataframes\n",
    "    X_full = pd.read_csv(os.path.join(args.train_data, \"train_X.csv\"))\n",
    "    y_full = pd.read_csv(os.path.join(args.train_data, \"train_Y.csv\"))\n",
    "\n",
    "    mlflow.start_run()\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    #Split training data in to training and validation sets\n",
    "    X_train, X_vali, y_train, y_vali =  train_test_split(X_full,y_full,test_size=args.test_train_ratio,random_state=args.random_state)\n",
    "    \n",
    "    #Holds the neural-network model from SkLearn\n",
    "    model_sklearnNN = MLPRegressor(\n",
    "        hidden_layer_sizes=args.hidden_layer_neurons, \n",
    "        activation=args.hidden_layer_activation, \n",
    "        solver=\"sgd\", \n",
    "        learning_rate=\"adaptive\", \n",
    "        learning_rate_init=args.learning_rate,  \n",
    "        max_iter=args.epochs,\n",
    "        momentum=args.momentum,\n",
    "        random_state=args.random_state\n",
    "        )\n",
    "\n",
    "    #A Scaler is defined according the scaling type provided and the final model pipeline consists\n",
    "    # of a scaler following by the neural network.\n",
    "    if (args.scaler_type == \"minmax\"):\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler_model_pipeline = Pipeline([\n",
    "            (\"scalar\", scaler),\n",
    "            (\"nn_model\", model_sklearnNN)\n",
    "            ])\n",
    "    elif (args.scaler_type == \"standard\"):\n",
    "        scaler = StandardScaler()\n",
    "        scaler_model_pipeline = Pipeline([\n",
    "            (\"scalar\", scaler),\n",
    "            (\"nn_model\", model_sklearnNN)\n",
    "            ])\n",
    "    elif (args.scaler_type == \"maxabs\"):\n",
    "        scaler = MaxAbsScaler()\n",
    "        scaler_model_pipeline = Pipeline([\n",
    "            (\"scalar\", scaler),\n",
    "            (\"nn_model\", model_sklearnNN)\n",
    "            ])\n",
    "    else:\n",
    "        scaler_model_pipeline = model_sklearnNN\n",
    "\n",
    "    scaler_model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    #Log training performance\n",
    "    y_predict = scaler_model_pipeline.predict(X_train)\n",
    "    mlflow.log_metric(\"Train RMSE\",  np.sqrt(mean_squared_error(y_train,y_predict)))\n",
    "    mlflow.log_metric(\"Train R2-score\", r2_score(y_train,y_predict))\n",
    "\n",
    "    #Log validation performance\n",
    "    y_predict = scaler_model_pipeline.predict(X_vali)\n",
    "    mlflow.log_metric(\"Validation RMSE\",  np.sqrt(mean_squared_error(y_vali,y_predict)))\n",
    "    mlflow.log_metric(\"Validation R2-score\", r2_score(y_vali,y_predict))\n",
    "\n",
    "    # Saving the model to a file\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=scaler_model_pipeline,\n",
    "        path=args.ouput_model_path,\n",
    "    )\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Code for the Component to Test and Register the Model\n",
    "The purpose of this component is to caculate the final test performance of the model using the holdout test set and register the model using MLFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {local_dir_Pipeline}/test_and_register_model_nnsklearn.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import mlflow.sklearn\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--test_data_folder\", type=str, help=\"Path of the test data folder\")\n",
    "    parser.add_argument(\"--input_model_path\", type=str, help=\"path of the input model\")\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"Name for the registered best model\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    X_test = pd.read_csv(os.path.join(args.test_data_folder, \"test_X.csv\"))\n",
    "    y_test = pd.read_csv(os.path.join(args.test_data_folder, \"test_Y.csv\"))\n",
    "\n",
    "    mlflow.start_run()\n",
    "    \n",
    "    loaded_best_model = mlflow.sklearn.load_model(args.input_model_path)\n",
    "\n",
    "    print(\"Registering the best sweeped model via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=loaded_best_model,\n",
    "        registered_model_name=args.registered_model_name,\n",
    "        artifact_path=args.registered_model_name,\n",
    "    )\n",
    "\n",
    "    y_predict = loaded_best_model.predict(X_test)\n",
    "    mlflow.log_metric(\"Test RMSE\",  np.sqrt(mean_squared_error(y_test,y_predict)))\n",
    "    mlflow.log_metric(\"Test R2-score\", r2_score(y_test,y_predict))\n",
    "\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Create Pipeline Components from the Code Definitions Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532156362
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "combine_data_component = command(\n",
    "    name=\"combine_files_for_train_test\",\n",
    "    display_name=\"combine_files_for_train_test\",\n",
    "    description=\"Combine .csv files for training and testing seperatly\",\n",
    "    inputs={\n",
    "        \"data_files\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    \n",
    "    code=local_dir_Pipeline,\n",
    "    command=\"python combine_data.py \\\n",
    "            --data_files ${{inputs.data_files}} \\\n",
    "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
    "            \",\n",
    "    environment=f\"{custom_env.name}:{custom_env.version}\"\n",
    ")\n",
    "combine_data_component = ml_client.create_or_update(combine_data_component.component)\n",
    "\n",
    "train_nnh1_component = command(\n",
    "    name=\"train_nnh1_regressor\",\n",
    "    display_name=\"train_nnh1_regressor\",\n",
    "    inputs={\n",
    "        \"random_state\": Input(type=\"number\"),\n",
    "        \"train_data\": Input(type=\"uri_folder\"),\n",
    "        \"test_train_ratio\": Input(type=\"number\"),\n",
    "        \"scaler_type\": Input(type=\"string\"),\n",
    "        \"hidden_layer_neurons\": Input(type=\"number\"),\n",
    "        \"hidden_layer_activation\": Input(type=\"string\"),\n",
    "        \"learning_rate\": Input(type=\"number\"),\n",
    "        \"epochs\": Input(type=\"number\"),\n",
    "        \"momentum\": Input(type=\"number\")\n",
    "    },\n",
    "    outputs={\n",
    "        \"ouput_model_path\": Output(type=\"mlflow_model\")\n",
    "    },\n",
    "    \n",
    "    code=local_dir_Pipeline,\n",
    "    command=\"python train_NNh1_sklearn.py \\\n",
    "            --random_state ${{inputs.random_state}} \\\n",
    "            --train_data ${{inputs.train_data}} \\\n",
    "            --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
    "            --scaler_type ${{inputs.scaler_type}}\\\n",
    "            --hidden_layer_neurons ${{inputs.hidden_layer_neurons}} \\\n",
    "            --hidden_layer_activation ${{inputs.hidden_layer_activation}} \\\n",
    "            --learning_rate ${{inputs.learning_rate}} \\\n",
    "            --epochs ${{inputs.epochs}} \\\n",
    "            --momentum ${{inputs.momentum}}\\\n",
    "            --ouput_model_path ${{outputs.ouput_model_path}} \\\n",
    "            \",\n",
    "    environment=f\"{custom_env.name}:{custom_env.version}\"\n",
    ")\n",
    "train_nnh1_component = ml_client.create_or_update(train_nnh1_component.component)\n",
    "\n",
    "test_and_register_model_component = command(\n",
    "    name=\"test_model\",\n",
    "    display_name=\"test_model\",\n",
    "    inputs={\n",
    "        \"test_data_folder\": Input(type=\"uri_folder\"),\n",
    "        \"input_model_path\": Input(type=\"mlflow_model\"),\n",
    "        \"registered_model_name\": Input(type=\"string\")\n",
    "    },\n",
    "    \n",
    "    code=local_dir_Pipeline,\n",
    "    command=\"python test_and_register_model_nnsklearn.py \\\n",
    "            --test_data_folder ${{inputs.test_data_folder}} \\\n",
    "            --input_model_path ${{inputs.input_model_path}} \\\n",
    "            --registered_model_name ${{inputs.registered_model_name}} \\\n",
    "            \",\n",
    "    environment=f\"{custom_env.name}:{custom_env.version}\"\n",
    ")\n",
    "test_and_register_model_component = ml_client.create_or_update(test_and_register_model_component.component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 Define the Pipeline\n",
    "Inputs for the pipeline are the following.\n",
    "1. input_data_folder : Contains the Data .csv files\n",
    "2. test_train_ratio : Used for the split within model training component\n",
    "3. registered_model_name : To register the best model\n",
    "4. random_state : Used within model training component for the train test split and the neural-network.\n",
    "\n",
    "Pipeline contains a hyper-parameter tuning step using \"sweep\". Best model from the sweep step will be registered and tested by the \"test_and_register_model_component\" of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532156879
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl\n",
    "from azure.ai.ml.sweep import Choice\n",
    "import numpy as np\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=compute_cluster_name, # to use serverless compute, change this to: compute=azureml:serverless\n",
    "    description=\"Gas Emission Prediction Pipeline - NNh1\",\n",
    ")\n",
    "def GassEmission_prediction_pipeline_nnh1_sklearn(\n",
    "    pipeline_job_input_data_folder,\n",
    "    pipeline_job_test_train_ratio,\n",
    "    pipeline_job_registered_model_name,\n",
    "    pipeline_job_random_state\n",
    "):\n",
    "    \n",
    "    combine_data_job = combine_data_component(\n",
    "        data_files=pipeline_job_input_data_folder\n",
    "    )\n",
    "\n",
    "    train_job = train_nnh1_component(\n",
    "        random_state=pipeline_job_random_state,\n",
    "        train_data=combine_data_job.outputs.train_data,\n",
    "        test_train_ratio=pipeline_job_test_train_ratio,\n",
    "        scaler_type=Choice(values=[\"minmax\", \"none\"]),\n",
    "        hidden_layer_neurons=50,\n",
    "        hidden_layer_activation=\"relu\",\n",
    "        learning_rate=Choice(values=[0.1, 0.2]),\n",
    "        epochs=Choice(values=[160, 240]),\n",
    "        momentum=0\n",
    "    )\n",
    "\n",
    "    sweep_step = train_job.sweep(\n",
    "        sampling_algorithm=\"grid\",\n",
    "        primary_metric=\"Validation R2-score\",\n",
    "        goal=\"maximize\"\n",
    "    )\n",
    "    sweep_step.set_limits(max_total_trials=500, max_concurrent_trials=20, timeout=72000)\n",
    "\n",
    "    test_and_register_model_job = test_and_register_model_component(\n",
    "        test_data_folder=combine_data_job.outputs.test_data,\n",
    "        registered_model_name=pipeline_job_registered_model_name,\n",
    "        input_model_path=sweep_step.outputs.ouput_model_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7 Create an Instance of the Pipeline and Run\n",
    "Input data folder should be local to the compute running this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532857294
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "registered_best_model_name = \"pipeline_best_model_nnsklearn\"\n",
    "\n",
    "pipeline = GassEmission_prediction_pipeline_nnh1_sklearn(\n",
    "    pipeline_job_random_state=0,\n",
    "    pipeline_job_input_data_folder=Input(type=\"uri_folder\", path=\"./Data/PipelineNN\"),\n",
    "    pipeline_job_test_train_ratio=0.2,\n",
    "    pipeline_job_registered_model_name=registered_best_model_name\n",
    ")\n",
    "\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    experiment_name=\"train_model_GasEmission_prediction_Pipeline_nnh1_sklearn\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create a Reference to the Trained Best Model\n",
    "This reference will then be the model deployment. It should be noted that some other model training process may register a model in the same name if run parallelly. To avoid such situations, name of the job that created the best model is cross checked with the pipeline job name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532859169
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "#Get the latest model with name given by registered_best_model_name\n",
    "latest_model_version = max(\n",
    "    [int(m.version) for m in ml_client.models.list(name=registered_best_model_name)]\n",
    ")\n",
    "registered_best_model = ml_client.models.get(name=registered_best_model_name,version=latest_model_version)\n",
    "\n",
    "#Get a reference to the job that created the latest model with name given by registered_best_model_name\n",
    "best_model_job = ml_client.jobs.get(name=registered_best_model.job_name)\n",
    "\n",
    "#Get a reference to the MLFlow run associated wih the job that created the latest model\n",
    "MLflow_client = MlflowClient()\n",
    "mlflow_best_model_job = MLflow_client.get_run(best_model_job.name)\n",
    "\n",
    "#Get a reference to its MLFlow parent run\n",
    "mlflow_best_model_job_parent = MLflow_client.get_run(mlflow_best_model_job.data.tags[\"mlflow.parentRunId\"])\n",
    "\n",
    "#If the MLflow parent run of the job that created the lated model with name  given by  registered_best_model_name is\n",
    "#not the pipeline job name, then there is a version conflict, which indicates that some other job has created a model with same name\n",
    "if (mlflow_best_model_job_parent.data.tags[\"mlflow.rootRunId\"] != pipeline_job.name):\n",
    "    print(\"Registered Best Model Version Conflict Detected! \\nPipeline_job runID and the best model's associated job's parent job's runID are mismatched. The model may be updated by some other job\")\n",
    "    registered_best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687532859617
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(\"Registered Best Model Name: \", registered_best_model.name)\n",
    "print(\"Registered Best Model Version: \", registered_best_model.version)\n",
    "print(\"Registered Best Model Path: \", registered_best_model.path)\n",
    "print(\"Registered Best Model ID: \", registered_best_model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Deployment as a Managed Online Deployment\n",
    "\n",
    "#### 7.1 Create the End-Point for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687536570116
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "#End-point name has to be unique\n",
    "endpoint_name = \"gaspredict-nnsklearn-\" + str(uuid.uuid4())[:8]\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"This is an online endpoint for CO gas emission prediction - nnsklearn\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\n",
    "        \"training_dataset\": \"sensor_data\",\n",
    "        \"model_type\": \"sklearn.MLPRegressor\",\n",
    "    },\n",
    ")\n",
    "\n",
    "endpoint_result = ml_client.begin_create_or_update(endpoint).result()\n",
    "\n",
    "print(f\"Endpint {endpoint_result.name} provisioning state: {endpoint_result.provisioning_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Create the Deployment inside the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687537314921
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=registered_best_model,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "blue_deployment_results = ml_client.online_deployments.begin_create_or_update(\n",
    "    blue_deployment\n",
    ").result()\n",
    "\n",
    "print(\n",
    "    f\"Deployment {blue_deployment_results.name} provisioning state: {blue_deployment_results.provisioning_state}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Test the Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a folder is created to hold the file containing the testing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "deploy_dir = \"./deploy\"\n",
    "if not os.path.exists(deploy_dir):\n",
    "    os.mkdir(deploy_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A .json file containing the samples is written. Samples are extracted from the hold-out test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {deploy_dir}/sample-request.json\n",
    "{\n",
    "  \"input_data\": {\n",
    "    \"columns\": [\"AT\", \"AP\", \"AH\", \"AFDP\", \"GTEP\", \"TIT\", \"TAT\", \"TEY\", \"CDP\"],\n",
    "    \"index\": [0, 1],\n",
    "    \"data\": [\n",
    "            [1.9532,1020.1,84.985,2.5304,20.116,1048.7,544.92,116.27,10.799],\n",
    "            [1.2191,1020.1,87.523,2.3937,18.584,1045.5,548.5,109.18,10.347]\n",
    "        ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the End-point with the Sample File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1687539692801
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=endpoint_name,\n",
    "    request_file=\"./deploy/sample-request.json\",\n",
    "    deployment_name=\"blue\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
